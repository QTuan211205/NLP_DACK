import os
import json
import time
import nltk
import warnings
import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

# Th∆∞ vi·ªán t√≠nh to√°n ƒëi·ªÉm
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge import Rouge
from nltk.translate.meteor_score import meteor_score

# Th∆∞ vi·ªán AI
from langchain_google_genai import ChatGoogleGenerativeAI

# T·∫Øt c·∫£nh b√°o
warnings.filterwarnings("ignore")

# Load m√¥i tr∆∞·ªùng
load_dotenv("key.env")
google_api_key = os.getenv("GOOGLE_API_KEY")

# Kh·ªüi t·∫°o Model Gemini
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash", # Ho·∫∑c gemini-pro t√πy t√†i kho·∫£n c·ªßa b·∫°n
    temperature=0, # Gi·ªØ temperature th·∫•p ƒë·ªÉ ƒë√°nh gi√° t√≠nh ch√≠nh x√°c
    google_api_key=google_api_key
)

def get_gemini(text):
    # Tr√≠ch xu·∫•t n·ªôi dung t·ª´ AIMessage object
    response = llm.invoke([text])
    return response.content

def call_model_with_retry(model_func, prompt):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            return model_func(prompt)
        except Exception as e:
            print(f"L·ªói: {e}. ƒêang th·ª≠ l·∫°i l·∫ßn {attempt+1}...")
            time.sleep(2)
    return ""

# T·∫£i t√†i nguy√™n NLTK
nltk.download("wordnet")
nltk.download("punkt")

# Kh·ªüi t·∫°o ROUGE
rouge = Rouge()
smoothing_function = SmoothingFunction().method1

def get_scores(hypothesis, reference):
    if not hypothesis or not reference:
        return 0, 0, 0
    
    # Tokenize cho BLEU v√† METEOR
    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())
    reference_tokens = nltk.word_tokenize(reference.lower())
    
    # BLEU Score
    bleu = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function)
    
    # ROUGE Score (S·ª≠ d·ª•ng chu·ªói vƒÉn b·∫£n g·ªëc)
    try:
        rouge_scores = rouge.get_scores(hypothesis.lower(), reference.lower())
        rouge_score = rouge_scores[0]["rouge-l"]["f"]
    except:
        rouge_score = 0
        
    # METEOR Score
    meteor = meteor_score([reference_tokens], hypothesis_tokens)
    
    return bleu, rouge_score, meteor

# ƒê∆∞·ªùng d·∫´n file v√† th∆∞ m·ª•c
results_dir = "results"
logs_dir = "logs"
os.makedirs(results_dir, exist_ok=True)
os.makedirs(logs_dir, exist_ok=True)

# ============================
# C·∫§U H√åNH DATASET
# ============================

DATASETS = {
    "1-hop": "data/benchmark/1hop.json",
    "2-hop": "data/benchmark/2hop.json",
}

test_limit = 200

# ============================
# H√ÄM CH·∫†Y EVALUATION
# ============================

def run_zero_shot(dataset_name, file_path):
    if not os.path.exists(file_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {file_path}")
        return

    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)

    print(f"\nüöÄ B·∫Øt ƒë·∫ßu Zero-shot {dataset_name} ({min(test_limit, len(data))} c√¢u h·ªèi)")

    scores = {"BLEU": [], "ROUGE": [], "METEOR": []}
    logs = []
    inference_times = []

    for x in tqdm(data[:test_limit], desc=f"{dataset_name}"):

        PROMPT = f"""
        B·∫°n l√† m·ªôt d∆∞·ª£c sƒ© l√¢m s√†ng v√† chuy√™n gia v·ªÅ D∆∞·ª£c ƒëi·ªÉn Vi·ªát Nam. 
        H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch ch√≠nh x√°c, ng·∫Øn g·ªçn v√† d·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n y d∆∞·ª£c.

        - Tr·∫£ l·ªùi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
        - Gi·ªØ ƒë·ªô ch√≠nh x√°c cao v·ªÅ t√™n thu·ªëc v√† c√¥ng th·ª©c h√≥a h·ªçc.
        
        C√¢u h·ªèi: {x["question"]}
        """

        start_time = time.time()
        gemini_result = call_model_with_retry(get_gemini, PROMPT)
        end_time = time.time()

        inference_times.append(end_time - start_time)

        reference = x["answer"]

        bleu, rouge_val, meteor = get_scores(gemini_result, reference)

        scores["BLEU"].append(bleu)
        scores["ROUGE"].append(rouge_val)
        scores["METEOR"].append(meteor)

        logs.append({
            "hop_type": dataset_name,
            "question": x["question"],
            "ground_truth": reference,
            "model_answer": gemini_result,
            "BLEU": bleu,
            "ROUGE": rouge_val,
            "METEOR": meteor,
            "time": end_time - start_time
        })

    # ============================
    # GHI K·∫æT QU·∫¢
    # ============================

    avg_time = sum(inference_times) / len(inference_times)

    result_path = os.path.join(results_dir, f"gemini_zero_shot_{dataset_name}.txt")
    log_path = os.path.join(logs_dir, f"gemini_zero_shot_{dataset_name}.json")

    with open(result_path, "w", encoding="utf-8") as f:
        f.write(f"{dataset_name} Zero-shot Results\n")
        f.write(f"Average inference time: {avg_time:.2f} seconds\n\n")
        for metric, values in scores.items():
            f.write(f"{metric}: {sum(values)/len(values):.4f}\n")

    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(logs, f, ensure_ascii=False, indent=4)

    print(f"‚úÖ Ho√†n th√†nh {dataset_name} | Avg time: {avg_time:.2f}s")
    print(f"üìÑ Results: {result_path}")
    print(f"üßæ Logs: {log_path}")

# ============================
# CH·∫†Y C·∫¢ 1-HOP & 2-HOP
# ============================

for hop_name, path in DATASETS.items():
    run_zero_shot(hop_name, path)
