import requests
import os
import json
import time
from dotenv import load_dotenv

# --- IMPORTS ---
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge import Rouge
from nltk.translate.meteor_score import meteor_score

# T·∫£i d·ªØ li·ªáu wordnet n·∫øu ch∆∞a c√≥
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download("wordnet")

from langchain_community.graphs import Neo4jGraph
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.chains import GraphCypherQAChain
import google.generativeai as genai

# Kh·ªüi t·∫°o Rouge
rouge = Rouge()

# ==============================================================================
# 1. C·∫§U H√åNH & K·∫æT N·ªêI
# ==============================================================================

current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, '..', 'key.env')
load_dotenv(env_path)

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
NEO4J_URI = os.getenv("URI", "neo4j://127.0.0.1:7687")
NEO4J_USER = os.getenv("USER", "neo4j")
NEO4J_PASSWORD = os.getenv("PASSWORD", "12345678")

if not GOOGLE_API_KEY:
    print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY.")
    exit()

genai.configure(api_key=GOOGLE_API_KEY)
MODEL_NAME = "gemini-2.0-flash" 

try:
    graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASSWORD)
    graph.refresh_schema()
    print("‚úÖ ƒê√£ k·∫øt n·ªëi Neo4j!")
except Exception as e:
    print(f"‚ùå L·ªói k·∫øt n·ªëi Neo4j: {e}")
    exit()

# ==============================================================================
# 2. PROMPT & SCHEMA
# ==============================================================================

examples = [
    # --- 1-HOP (CƒÉn b·∫£n) ---
    {
        "question": "C√¥ng th·ª©c h√≥a h·ªçc c·ªßa Aspirin l√† g√¨?",
        "query": "MATCH (n:HO·∫†T_CH·∫§T) WHERE toLower(n.t√™n_ho·∫°t_ch·∫•t) CONTAINS toLower('ASPIRIN') RETURN n.t√™n_ho·∫°t_ch·∫•t, n.c√¥ng_th·ª©c_h√≥a_h·ªçc",
    },
    # --- 2-HOP (X√© nh·ªè t·ª´ kh√≥a t√≠nh ch·∫•t) ---
    {
        "question": "D∆∞·ª£c ch·∫•t c√≥ t√≠nh ch·∫•t [b·ªôt tr·∫Øng, tan trong n∆∞·ªõc, kh√¥ng tan trong ethanol] c√≥ ƒë·ªãnh t√≠nh l√† g√¨?",
        "query": "MATCH (n:HO·∫†T_CH·∫§T)-[:C√ì_TI√äU_CHU·∫®N]->(t:TI√äU_CHU·∫®N) WHERE toLower(n.t√≠nh_ch·∫•t) CONTAINS 'b·ªôt' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'tr·∫Øng' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'n∆∞·ªõc' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'ethanol' RETURN n.t√™n_ho·∫°t_ch·∫•t, t.ƒë·ªãnh_t√≠nh, t.ƒë·ªãnh_l∆∞·ª£ng",
    },
    {
        "question": "Quy tr√¨nh ƒë·ªãnh t√≠nh cho d∆∞·ª£c ch·∫•t c√≥ ƒë·∫∑c t√≠nh [tnc ~143¬∞C, d·ªÖ tan trong n∆∞·ªõc v√† ethanol]?",
        "query": "MATCH (n:HO·∫†T_CH·∫§T)-[:C√ì_TI√äU_CHU·∫®N]->(t:TI√äU_CHU·∫®N) WHERE n.t√≠nh_ch·∫•t CONTAINS '143' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'n∆∞·ªõc' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'ethanol' RETURN n.t√™n_ho·∫°t_ch·∫•t, t.ƒë·ªãnh_t√≠nh, t.ƒë·ªô_h√≤a_tan",
    },
    {
        "question": "T√¨m ho·∫°t ch·∫•t l√† [tinh th·ªÉ kh√¥ng m√†u, kh√≥ tan trong n∆∞·ªõc] v√† thu·ªôc lo·∫°i thu·ªëc g√¨?",
        "query": "MATCH (n:HO·∫†T_CH·∫§T)-[:THU·ªòC_NH√ìM]->(l:LO·∫†I_THU·ªêC) WHERE toLower(n.t√≠nh_ch·∫•t) CONTAINS 'tinh th·ªÉ' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'kh√¥ng m√†u' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'kh√≥ tan' RETURN n.t√™n_ho·∫°t_ch·∫•t, l.t√™n_lo·∫°i",
    },
    {
        "question": "X√°c ƒë·ªãnh ho·∫°t ch·∫•t c√≥ t√≠nh ch·∫•t [b·ªôt k·∫øt tinh tr·∫Øng, ƒëa h√¨nh, ƒë·ªô tan th·∫•p]?",
        "query": "MATCH (n:HO·∫†T_CH·∫§T) WHERE toLower(n.t√≠nh_ch·∫•t) CONTAINS 'b·ªôt' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'tr·∫Øng' AND toLower(n.t√≠nh_ch·∫•t) CONTAINS 'ƒëa h√¨nh' RETURN n.t√™n_ho·∫°t_ch·∫•t, n.t√™n_latin, n.c√¥ng_th·ª©c_h√≥a_h·ªçc",
    }
]

# C·∫≠p nh·∫≠t PREFIX v·ªõi h∆∞·ªõng d·∫´n x√© nh·ªè t·ª´ kh√≥a c·ª±c k·ª≥ quan tr·ªçng
PREFIX = """
B·∫°n l√† m·ªôt chuy√™n gia v·ªÅ c∆° s·ªü d·ªØ li·ªáu ƒë·ªì th·ªã Neo4j. Nhi·ªám v·ª• c·ªßa b·∫°n l√† chuy·ªÉn ƒë·ªïi c√¢u h·ªèi Ti·∫øng Vi·ªát th√†nh truy v·∫•n Cypher ch√≠nh x√°c.

C·∫•u tr√∫c c∆° s·ªü d·ªØ li·ªáu:
1. Node: HO·∫†T_CH·∫§T (t√™n_ho·∫°t_ch·∫•t, t√™n_latin, c√¥ng_th·ª©c_h√≥a_h·ªçc, m√¥_t·∫£, b·∫£o_qu·∫£n, t√≠nh_ch·∫•t)
2. Node: TI√äU_CHU·∫®N (ƒë·ªãnh_l∆∞·ª£ng, ƒë·ªãnh_t√≠nh, ƒë·ªô_h√≤a_tan, t·∫°p_ch·∫•t_v√†_ƒë·ªô_tinh_khi·∫øt, h√†m_l∆∞·ª£ng_y√™u_c·∫ßu)
   - Quan h·ªá: (:HO·∫†T_CH·∫§T)-[:C√ì_TI√äU_CHU·∫®N]->(:TI√äU_CHU·∫®N)
3. Node: LO·∫†I_THU·ªêC (t√™n_lo·∫°i)
   - Quan h·ªá: (:HO·∫†T_CH·∫§T)-[:THU·ªòC_NH√ìM]->(:LO·∫†I_THU·ªêC)

H∆Ø·ªöNG D·∫™N CHI·∫æN THU·∫¨T QUAN TR·ªåNG:
- LU√îN S·ª¨ D·ª§NG `toLower()`: ƒê·ªÉ t√¨m ki·∫øm kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng.
- CHI·∫æN THU·∫¨T X√â NH·ªé (KEYWORD SHREDDING): ƒê·ªëi v·ªõi c√°c m√¥ t·∫£ trong ngo·∫∑c [ ], TUY·ªÜT ƒê·ªêI KH√îNG s·ª≠ d·ª•ng nguy√™n vƒÉn c·∫£ chu·ªói d√†i. H√£y t√°ch th√†nh c√°c t·ª´ kh√≥a ƒë∆°n l·∫ª v√† n·ªëi b·∫±ng `AND`.
- ∆ØU TI√äN S·ªê LI·ªÜU: N·∫øu trong m√¥ t·∫£ c√≥ s·ªë (nhi·ªát ƒë·ªô n√≥ng ch·∫£y, ƒëi·ªÉm ch·∫£y), h√£y ƒë∆∞a s·ªë ƒë√≥ v√†o truy v·∫•n v√¨ n√≥ gi√∫p ƒë·ªãnh danh ch√≠nh x√°c nh·∫•t.
- TR·∫¢ V·ªÄ ƒêA TR∆Ø·ªúNG: Khi h·ªèi v·ªÅ 'ƒë·ªãnh t√≠nh' ho·∫∑c 'quy tr√¨nh', h√£y RETURN c·∫£ ƒë·ªãnh_t√≠nh, ƒë·ªãnh_l∆∞·ª£ng v√† ƒë·ªô_h√≤a_tan ƒë·ªÉ ƒë·ªÅ ph√≤ng d·ªØ li·ªáu b·ªã l·ªách c·ªôt.
"""

example_prompt = PromptTemplate.from_template("User input: {question}\nCypher query: {query}")

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=PREFIX,
    suffix="User input: {question}\nCypher query: ",
    input_variables=["question"],
)

gemini_chain = GraphCypherQAChain.from_llm(
    ChatGoogleGenerativeAI(model=MODEL_NAME, google_api_key=GOOGLE_API_KEY, temperature=0),
    graph=graph,
    verbose=True,
    cypher_prompt=prompt,
    allow_dangerous_requests=True
)

# ==============================================================================
# 3. CHU·∫®N B·ªä D·ªÆ LI·ªÜU
# ==============================================================================

data_test_1_hop =[]
data_test_2_hop=[]

# ==============================================================================
# 4. H√ÄM ƒê√ÅNH GI√Å (EVALUATION FUNCTION)
# ==============================================================================

chen_smoothing = SmoothingFunction().method1

def run_evaluation(dataset, label_name):
    """
    Ch·∫°y ƒë√°nh gi√° cho m·ªôt b·ªô d·ªØ li·ªáu c·ª• th·ªÉ.
    Tr·∫£ v·ªÅ: (k·∫øt qu·∫£ trung b√¨nh dict, danh s√°ch logs chi ti·∫øt)
    """
    print(f"\nüöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y TH·ª¨ NGHI·ªÜM: {label_name.upper()} ({len(dataset)} m·∫´u)")
    
    total_bleu = 0
    total_rouge = 0
    total_meteor = 0
    local_logs = []

    for i, x in enumerate(dataset):
        print(f"\nüîπ [{label_name}] C√¢u h·ªèi {i+1}: {x['question']}")
        
        # G·ªçi Gemini Chain
        try:
            response = gemini_chain.invoke(x["question"])
            gemini_result = response.get('result', str(response))
        except Exception as e:
            gemini_result = "Kh√¥ng t√¨m th·∫•y trong DB."
        
        if "I don't know" in str(gemini_result) or not gemini_result:
            gemini_result = "Kh√¥ng t√¨m th·∫•y trong DB."
        
        print(f"‚úÖ Tr·∫£ l·ªùi: {gemini_result}")

        # T√≠nh ƒëi·ªÉm
        reference = x["answer"]
        candidate = gemini_result
        ref_tokens = reference.split()
        cand_tokens = candidate.split()

        # BLEU
        b_score = sentence_bleu([ref_tokens], cand_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=chen_smoothing)
        
        # ROUGE
        try:
            if not candidate.strip(): r_score = 0
            else: r_score = rouge.get_scores(candidate, reference)[0]['rouge-l']['f']
        except: r_score = 0
        
        # METEOR
        try: m_score = meteor_score([ref_tokens], cand_tokens)
        except: m_score = 0

        total_bleu += b_score
        total_rouge += r_score
        total_meteor += m_score

        print(f"üìä ƒêi·ªÉm: BLEU={b_score:.2f} | ROUGE={r_score:.2f} | METEOR={m_score:.2f}")

        local_logs.append({
            "type": label_name,
            "question": x["question"],
            "answer_ground_truth": reference,
            "answer_model": candidate,
            "scores": {"bleu": b_score, "rouge": r_score, "meteor": m_score}
        })
        
        time.sleep(1) # Delay nh·∫π tr√°nh rate limit

    # T√≠nh trung b√¨nh
    n = len(dataset)
    if n > 0:
        avg_results = {
            "bleu": total_bleu / n,
            "rouge": total_rouge / n,
            "meteor": total_meteor / n,
            "count": n
        }
    else:
        avg_results = {"bleu": 0, "rouge": 0, "meteor": 0, "count": 0}

    return avg_results, local_logs

# ======================================================================
# LOAD DATASET T·ª™ FILE JSON
# ======================================================================

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data", "benchmark")

path_1hop = os.path.join(DATA_DIR, "1hop.json")
path_2hop = os.path.join(DATA_DIR, "2hop.json")

def load_json_data(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

data_test_1_hop = load_json_data(path_1hop)
data_test_2_hop = load_json_data(path_2hop)
MAX_QUESTIONS = 200

data_test_1_hop = load_json_data(path_1hop)[:MAX_QUESTIONS]
data_test_2_hop = load_json_data(path_2hop)[:MAX_QUESTIONS]

print(f"‚úÖ 1-hop: ch·∫°y {len(data_test_1_hop)} c√¢u h·ªèi")
print(f"‚úÖ 2-hop: ch·∫°y {len(data_test_2_hop)} c√¢u h·ªèi")

# ==============================================================================
# 5. CH·∫†Y TH·ª∞C NGHI·ªÜM V√Ä GHI FILE
# ==============================================================================

results_dir = "results"
logs_dir = "logs"
os.makedirs(results_dir, exist_ok=True)
os.makedirs(logs_dir, exist_ok=True)

gemini_results_path = os.path.join(results_dir, "gemini_results.txt")
gemini_log_path = os.path.join(logs_dir, "gemini_log.json")

# --- CH·∫†Y L·∫¶N L∆Ø·ª¢T 2 B·ªò DATA ---
avg_1_hop, logs_1_hop = run_evaluation(data_test_1_hop, "1-hop")
avg_2_hop, logs_2_hop = run_evaluation(data_test_2_hop, "2-hop")

# T·ªïng h·ª£p log
full_logs = {
    "1_hop_data": logs_1_hop,
    "2_hop_data": logs_2_hop
}

# --- IN K·∫æT QU·∫¢ RA M√ÄN H√åNH ---
print("\n" + "="*50)
print("üèÜ T·ªîNG H·ª¢P K·∫æT QU·∫¢ BENCHMARK")
print("="*50)
print(f"üîπ 1-HOP ({avg_1_hop['count']} m·∫´u):")
print(f"   BLEU: {avg_1_hop['bleu']:.4f} | ROUGE-L: {avg_1_hop['rouge']:.4f} | METEOR: {avg_1_hop['meteor']:.4f}")
print("-" * 50)
print(f"üîπ 2-HOP ({avg_2_hop['count']} m·∫´u):")
print(f"   BLEU: {avg_2_hop['bleu']:.4f} | ROUGE-L: {avg_2_hop['rouge']:.4f} | METEOR: {avg_2_hop['meteor']:.4f}")
print("="*50)

# --- GHI FILE RESULTS TXT ---
with open(gemini_results_path, "w", encoding='utf-8') as f:
    f.write("B√ÅO C√ÅO K·∫æT QU·∫¢ BENCHMARK (PH√ÇN LO·∫†I HOP)\n")
    f.write(f"Th·ªùi gian ch·∫°y: {time.ctime()}\n")
    f.write("==================================================\n\n")
    
    f.write(f"1. K·∫æT QU·∫¢ 1-HOP (S·ªë m·∫´u: {avg_1_hop['count']})\n")
    f.write(f"   - BLEU Score    : {avg_1_hop['bleu']:.4f}\n")
    f.write(f"   - ROUGE-L Score : {avg_1_hop['rouge']:.4f}\n")
    f.write(f"   - METEOR Score  : {avg_1_hop['meteor']:.4f}\n\n")
    
    f.write("--------------------------------------------------\n\n")

    f.write(f"2. K·∫æT QU·∫¢ 2-HOP (S·ªë m·∫´u: {avg_2_hop['count']})\n")
    f.write(f"   - BLEU Score    : {avg_2_hop['bleu']:.4f}\n")
    f.write(f"   - ROUGE-L Score : {avg_2_hop['rouge']:.4f}\n")
    f.write(f"   - METEOR Score  : {avg_2_hop['meteor']:.4f}\n")
    
    f.write("\n==================================================")

print(f"üéâ ƒê√£ l∆∞u b√°o c√°o t√≥m t·∫Øt v√†o: {gemini_results_path}")

# --- GHI FILE LOG JSON ---
with open(gemini_log_path, "w", encoding='utf-8') as f:
    json.dump(full_logs, f, ensure_ascii=False, indent=4)
print(f"üéâ ƒê√£ l∆∞u log chi ti·∫øt v√†o: {gemini_log_path}")